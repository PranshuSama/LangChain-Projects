{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1cf9edc",
   "metadata": {},
   "source": [
    "# Quickstart with RAGStack\n",
    "\n",
    "This notebook demonstrates how to set up a simple RAG pipeline with RAGStack. At the end of this notebook, you will have a fully functioning Question/Answer model that can answer questions using your supplied documents.\n",
    "\n",
    "A RAG pipeline requires, at minimum, a vector store, an embedding model, and an LLM. In this tutorial, you will use an Astra DB vector store, an OpenAI embedding model, an OpenAI LLM, and LangChain to orchestrate it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569235f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q ragstack-ai datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e5d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade langchain-core langchain langserve langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f9f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#   \"clientId\": \"DTISDoqYjaOriMsNubisQKXp\",\n",
    "#   \"secret\": \"xCwq33CYToeiYEaDEW+kkYfPXO.h6Y4Tr-.e9WYfKw.WQu-y9bAMtu.+orE3oqtWtRS4DkZG+2718CAt-nN,1gyICM69+GZj,9xz0ujtQP.DYiNj9KU54Kzax4UuM3yn\",\n",
    "#   \"token\": \"\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c97860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassio\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ASTRA_DB_API_ENDPOINT = os.getenv(\"ASTRA_DB_API_ENDPOINT\")\n",
    "ASTRA_DB_APPLICATION_TOKEN = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7be648",
   "metadata": {},
   "source": [
    "## Create RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431bb9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "from langchain_core.embeddings import Embeddings\n",
    "import os\n",
    "\n",
    "# Use SentenceTransformer directly for embeddings (avoids langchain_community import issues)\n",
    "class CustomHuggingFaceEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts).tolist()\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode([text])[0].tolist()\n",
    "\n",
    "embedding = CustomHuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(f\"Using embedding model: {type(embedding).__name__}\")\n",
    "\n",
    "vstore = AstraDBVectorStore(\n",
    "    collection_name=\"test\",\n",
    "    embedding=embedding,\n",
    "    token=os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\"),\n",
    "    api_endpoint=os.getenv(\"ASTRA_DB_API_ENDPOINT\"),\n",
    ")\n",
    "print(\"Astra vector store configured with HuggingFaceEmbeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6003ddb3",
   "metadata": {},
   "source": [
    "This code block replaces the `OpenAIEmbeddings` with `HuggingFaceEmbeddings` using the `all-MiniLM-L6-v2` model, which will be loaded locally. Make sure you have the `sentence-transformers` library installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73fb0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a sample dataset\n",
    "philo_dataset = load_dataset(\"datastax/philosopher-quotes\")[\"train\"]\n",
    "print(\"An example entry:\")\n",
    "print(philo_dataset[16])\n",
    "\n",
    "# Search for quotes about sleep, brain, or restore\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Searching for quotes about sleep, brain, or restore...\")\n",
    "print(\"=\"*80)\n",
    "keywords = [\"sleep\", \"brain\", \"restore\"]\n",
    "for keyword in keywords:\n",
    "    matching = [q for q in philo_dataset if keyword.lower() in q[\"quote\"].lower()]\n",
    "    print(f\"\\nQuotes containing '{keyword}': {len(matching)}\")\n",
    "    for quote in matching[:3]:  # Show first 3\n",
    "        print(f\"  - {quote['quote']}\")\n",
    "        print(f\"    Author: {quote['author']}, Tags: {quote['tags']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350cd5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Constructs a set of documents from your data. Documents can be used as inputs to your vector store.\n",
    "docs = []\n",
    "for entry in philo_dataset:\n",
    "    metadata = {\"author\": entry[\"author\"]}\n",
    "    if entry[\"tags\"]:\n",
    "        # Add metadata tags to the metadata dictionary\n",
    "        for tag in entry[\"tags\"].split(\";\"):\n",
    "            metadata[tag] = \"y\"\n",
    "    # Create a LangChain document with the quote and metadata tags\n",
    "    doc = Document(page_content=entry[\"quote\"], metadata=metadata)\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e9c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if you can connect to Astra DB\n",
    "try:\n",
    "    from astrapy.db import AstraDB\n",
    "    db = AstraDB(\n",
    "        token=os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"],\n",
    "        api_endpoint=os.environ[\"ASTRA_DB_API_ENDPOINT\"]\n",
    "    )\n",
    "    print(\"Connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b51f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings by inserting your documents into the vector store.\n",
    "import time\n",
    "\n",
    "max_retries = 3\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        inserted_ids = vstore.add_documents(docs)\n",
    "        print(f\"\\nInserted {len(inserted_ids)} documents.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        if attempt < max_retries - 1:\n",
    "            print(f\"Attempt {attempt + 1} failed: {type(e).__name__}\")\n",
    "            print(f\"Retrying in 5 seconds...\")\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            print(f\"Failed after {max_retries} attempts: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7f348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks your collection to verify the documents are embedded.\n",
    "print(vstore.astra_db.collection(\"test\").find())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58d4a2b",
   "metadata": {},
   "source": [
    "### Basic Retrieval\n",
    "\n",
    "Retrieve context from your vector database, and pass it to the model with a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a3480",
   "metadata": {},
   "outputs": [],
   "source": [
    "## can us chatopenai but dont have any free api keys so using chatgroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42289a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from groq import Groq\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Get Groq API Key\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass(\"Enter your Groq API Key: \")\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "retriever = vstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Answer the question based only on the supplied context. If you don't know the answer, say you don't know the answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Your answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Execute RAG chain\n",
    "question = \"What is the importance of sleep for the brain?\"\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "print(\"-\" * 80)\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "formatted_prompt = prompt_template.format(context=context, question=question)\n",
    "\n",
    "# Call Groq API using groq Python client\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "result = response.choices[0].message.content\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RAG Response:\")\n",
    "print(\"=\"*80)\n",
    "print(result)\n",
    "\n",
    "# Extract and display tags from first retrieved document\n",
    "if retrieved_docs:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Tags from source:\")\n",
    "    print(\"=\"*80)\n",
    "    tags = [key for key in retrieved_docs[0].metadata.keys() if retrieved_docs[0].metadata[key] == 'y']\n",
    "    print(f\"Tags: {', '.join(tags)}\")\n",
    "    print(f\"Author: {retrieved_docs[0].metadata.get('author', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b30e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e3e51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
